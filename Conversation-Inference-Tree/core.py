from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, pipeline
import os
from dotenv import load_dotenv
import openai
from treelib import Node, Tree
import praw

#Todo List:
    #COMPLETE: Change Scraper to get the correct data entries(seems to be missing a couple datapoints for the comments)
    #
    #Add the code in the process function that brings together the query and the input
    #Figure out where the text input is coming from in this case
    #
    #Change the agent storage so that it accepts Agent object instead of sub-dictionaries
    #
    #Finish ProcessThread function in InferenceTree class
    #Clean up UnifyDatatype function in ConversationTree class
    #
    #Rename ConversationTree and InferenceTree to something more descriptive and differentiated
    #
    #Fill out dictionary in ConversationTree __init__ function
    #
    #


class InferenceTree:
    llm = None
    agent_dict = dict()
    agent_output_dict = dict()
    max_summary_nodes = 5
    load_dotenv()

    os.environ["TORCH_USE_CUDA_DSA"] = "1"
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

    def set_summarizer(instruction: str, max_nodes=5):
        agent = {
            "query": instruction
        }
        #adds the summarizer to the agent dictionary
        InferenceTree.agent_dict["_summarizer"] = agent

    def set_llm(model_type: str, model_name: str, model_parameters: dict):
        #sets the llm that will be used by the other functions, and exposes it as an accessible variable
        if model_type == "huggingface":
            #This code runs if the llm is from huggingface.co or a local huggingface model
            key = os.getenv('token')
            # user = os.getenv('username')

            #save the user-defined model hyperparameters to an AutoConfig object, then initialize the model into a global variable
            config = AutoConfig.from_pretrained(model_name, **model_parameters)
            automodel = AutoModelForCausalLM.from_pretrained(model_name, config=config)
            autotokenizer = AutoTokenizer.from_pretrained(model_name)
            InferenceTree.llm = pipeline("text-generation", model=automodel, tokenizer=autotokenizer)
        elif model_type == "openai":
            #This code runs if the llm is accessed throught the openai api
            key = os.getenv('OPENAI_API_KEY')
            InferenceTree.llm = {
                "model": model_name,
                "config": model_parameters
            }
    #process reddit thread
    def process_thread(data, data_type: str, input_location: str = "", output_location: str = ""):
        
        #If input_location is not equal to "", pull data from json files

        thread = _ConversationTree(data)
        #return final summary
        pass

class _Agent:
    global query
    def __init__(self, query):
        self.query = query

    def process(prompt, model): #--Handles the basic processing of all agents
        
        #create the prompt by bringing toghether the question the agent will ask(query), 
        # and the textual input the query is focused on.


        try:
            if isinstance(model, dict):
                response = openai.ChatCompletion.create(
                    model=model["model"],
                    message=prompt,
                    config=model["config"]
                )
                return response['choices'][0]['message']['content']
            else:
                response = model(prompt)
                return response#NOTE: check to see if this is the correct object to return
        except Exception as e:
            print(f"Error generating agent output: {e}")

class _ConversationTree:
    def __init__(self, submission):
        self.tree = Tree()

        submission.comments.replace_more(limit=None)

        # Add root node (submission itself)
        self.tree.create_node("root-node", submission.id, data={
            #NOTE: fill out dictionary
        })

        # Start recursion from top-level comments
        for top_level_comment in submission.comments:
            self._recursive_node(top_level_comment, submission.id)
    
    #sets the subcomments of entry as child nodes, and repeats the chain
    #does not handle setting the entry node itself, as that would make setting the root complicated
    def _recursive_node(self, entry):
        #entry will be a comment object with 0 or more subcomments

        if len(entry.replies) != 0:
            for child in entry.replies:
                self.tree.create_node(child.body[:30], child.id, parent=entry.id, data={
                    #NOTE: fill out dictionary
                })
                
                # self.tree = self._recursive_node(child)
                self._recursive_node(child)

#can take either a json dictionary entry for a comment, or a praw comment object
#Generated by ChatGPT
class _RedditWrapper:
    def __init__(self, source):
        """
        Initialize with either a PRAW Comment object or a dict (from JSON).
        Expected attributes: id, body, author, parent_id, depth
        """
        if isinstance(source, dict):
            self.id = source.get('id')
            self.body = source.get('body')
            self.author = source.get('author')
            self.parent_id = source.get('parent_id')
            self.depth = source.get('depth')
        
        elif hasattr(source, 'body') and hasattr(source, 'parent_id'):
            # Likely a Comment object
            self.id = source.id
            self.body = source.body
            self.author = str(source.author) if source.author else None
            self.parent_id = source.parent_id
            self.depth = getattr(source, 'depth', None)

        elif hasattr(source, 'title') and hasattr(source, 'selftext'):
            # Likely a Submission object
            self.id = source.id
            self.body = f"{source.title}\n\n{source.selftext}".strip()
            self.author = str(source.author) if source.author else None
            self.parent_id = None
            self.depth = 0

        else:
            raise TypeError("Unsupported source type for RedditItemWrapper")